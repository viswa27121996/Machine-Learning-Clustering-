'''
# K-Means Clustering Algorithm - Data Mining (Machine Learning) Unsupervised learning Algorithm

# Business Problem Statement:

# Students have to evaluate a lot of factors before taking a decision 
to join a university for their higher education requirements.

# `CRISP-ML(Q)` process model describes six phases:
# 1. Business and Data Understanding
# 2. Data Preparation
# 3. Model Building
# 4. Evaluation
# 5. Deployment
# 6. Monitoring and Maintenance
'''
# Objective(s): Maximize the convenience of admission process
# Constraints: Minimize the brain drain

'''Success Criteria'''

# Business Success Criteria: Reduce the application process time from anywhere between 20% to 40%
# ML Success Criteria: Achieve Silhoutte coefficient of atleast 0.5
# Economic Success Criteria: US Higher education department will see an increase in revenues by atleast 30%

# **Proposed Plan:**
# Grouping the available universities will allow to understand the characteristics of each group.

'''
# ## Data Collection

# Data: 
#    The university details are obtained from the US Higher Education Body and is publicly available for students to access.
# 
# Data Dictionary:
# - Dataset contains 25 university details
# - 8 features are recorded for each university
# 
# Description:
# - Univ - University Name
# - State - Location (state) of the university
# - SAT - Cutoff SAT score for eligibility
# - Top10 - % of students who ranked in top 10 in their previous academics
# - Accept - % of students admitted to the universities
# - SFRatio - Student to Faculty ratio
# - Expenses - Overall cost in USD
# - GradRate - % of students who graduate
'''

# #### Install the required packages if not available
# !pip install feature_engine
# !pip install sklearn_pandas

# **Importing required packages**
# import numpy as np
import pandas as pd 
import sweetviz
import matplotlib.pyplot as plt

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
# from sklearn.compose import ColumnTransformer
# from sklearn.preprocessing import OneHotEncoder

from sklearn.cluster import KMeans
from sklearn import metrics
import joblib
import pickle

# **Import the data**

from sqlalchemy import create_engine

uni = pd.read_excel(r"C:\Users\Bharani Kumar\Downloads\KMeans_material (1)\KMeans_material\University_Clustering.xlsx")

# Credentials to connect to Database
user = 'user1'  # user name
pw = 'user1'  # password
db = 'univkm_db'  # database name
engine = create_engine(f"mysql+pymysql://{user}:{pw}@localhost/{db}")

# to_sql() - function to push the dataframe onto a SQL table.
uni.to_sql('univ_tbl', con = engine, if_exists = 'replace', chunksize = 1000, index = False)

sql = 'select * from univ_tbl;'
df = pd.read_sql_query(sql, engine)

# Data types
df.info()

# Drop the unwanted features
df1 = df.drop(["UnivID", "Univ"], axis = 1)

df1.head()


# # EXPLORATORY DATA ANALYSIS (EDA) / DESCRIPTIVE STATISTICS

# ***Descriptive Statistics and Data Distribution Function***
df1.describe()

# ## We have to check unique values for categorical data 
df1.State.unique()
df1.State.unique().size

df1.State.value_counts()

# AutoEDA
# Automated Libraries
# import sweetviz
my_report = sweetviz.analyze([df1, "df1"])
my_report.show_html('Report.html')

# Missing Data
# Checking Null Values
df1.isnull().sum()

# Segregate Numeric and Non-numeric columns
df1.info()

# **By using Mean imputation null values can be impute**
numeric_features = df1.select_dtypes(exclude = ['object']).columns
numeric_features

# Non-numeric columns
categorical_features = df1.select_dtypes(include = ['object']).columns
categorical_features

# Define Pipeline to deal with Missing data and scaling numeric columns
num_pipeline = Pipeline([('impute', SimpleImputer(strategy = 'mean')), ('scale', MinMaxScaler())])
num_pipeline

# Fit the numeric data to the pipeline. Ignoring State column
processed = num_pipeline.fit(df1[numeric_features]) 

# Save the pipeline
joblib.dump(processed, 'processed1')

# Transform the data with pipeline on numberic columns to get clean data
univ_clean = pd.DataFrame(processed.transform(df1[numeric_features]), columns = numeric_features)
univ_clean

'''
# Encoding Non-numeric fields
# **Convert Categorical data "State" to Numerical data using OneHotEncoder**

categ_pipeline = Pipeline([('OnehotEncode', OneHotEncoder(drop = 'first'))])
categ_pipeline

# Using ColumnTransfer to transform the columns of an array or pandas DataFrame. 
# This estimator allows different columns or column subsets of the input to be
# transformed separately and the features generated by each transformer will
# be concatenated to form a single feature space.
preprocess_pipeline = ColumnTransformer([('categorical', categ_pipeline, categorical_features), 
                                       ('numerical', num_pipeline, numeric_features)], 
                                        remainder = 'passthrough') # Skips the transformations for remaining columns

preprocess_pipeline

# Pass the raw data through pipeline
processed2 = preprocess_pipeline.fit(df1) 


# ## Save the Imputation and Encoding pipeline
# import joblib
joblib.dump(processed2, 'processed2')

# File gets saved under current working directory
import os
os.getcwd()

# Clean and processed data for Clustering
univ = pd.DataFrame(processed2.transform(df1), columns = list(processed2.get_feature_names_out()))
univ
'''

# Clean data
univ_clean.describe()

# # CLUSTERING MODEL BUILDING

# ### KMeans Clustering
# Libraries for creating scree plot or elbow curve 
# from sklearn.cluster import KMeans
# import matplotlib.pyplot as plt

###### scree plot or elbow curve ############
TWSS = []
k = list(range(2, 9))

for i in k:
    kmeans = KMeans(n_clusters = i)
    kmeans.fit(univ_clean)
    TWSS.append(kmeans.inertia_)

TWSS

# ## Creating a scree plot to find out no.of cluster
plt.plot(k, TWSS, 'ro-'); plt.xlabel("No_of_Clusters"); plt.ylabel("total_within_SS")



# ## Using KneeLocator
List = []

for k in range(2, 9):
    kmeans = KMeans(n_clusters = k, init = "random", max_iter = 30, n_init = 10) 
    kmeans.fit(univ_clean)
    List.append(kmeans.inertia_)

# !pip install kneed
from kneed import KneeLocator
kl = KneeLocator(range(2, 9), List, curve = 'convex')
# kl = KneeLocator(range(2, 9), List, curve='convex', direction = 'decreasing')
kl.elbow
plt.style.use("seaborn")
plt.plot(range(2, 9), List)
plt.xticks(range(2, 9))
plt.ylabel("Interia")
plt.axvline(x = kl.elbow, color = 'r', label = 'axvline - full height', ls = '--')
plt.show() 

# Not able to detect the best K value (knee/elbow) as the line is mostly linear

# Building KMeans clustering
model = KMeans(n_clusters = 3)
yy = model.fit(univ_clean)

# Cluster labels
model.labels_


# ## Cluster Evaluation

# **Silhouette coefficient:**  
# Silhouette coefficient is a Metric, which is used for calculating 
# goodness of clustering technique and the value ranges between (-1 to +1).
# It tells how similar an object is to its own cluster (cohesion) compared to 
# other clusters (separation).
# A score of 1 denotes the best meaning that the data point is very compact 
# within the cluster to which it belongs and far away from the other clusters.
# Values near 0 denote overlapping clusters.

# from sklearn import metrics
metrics.silhouette_score(univ_clean, model.labels_)

# **Calinski Harabasz:**
# Higher value of CH index means cluster are well separated.
# There is no thumb rule which is acceptable cut-off value.
metrics.calinski_harabasz_score(univ_clean, model.labels_)

# **Davies-Bouldin Index:**
# Unlike the previous two metrics, this score measures the similarity of clusters. 
# The lower the score the better the separation between your clusters. 
# Vales can range from zero and infinity
metrics.davies_bouldin_score(univ_clean, model.labels_)

# ### Evaluation of Number of Clusters using Silhouette Coefficient Technique
from sklearn.metrics import silhouette_score

silhouette_coefficients = []

for k in range (2, 9):
    kmeans = KMeans(n_clusters = k)
    kmeans.fit(univ_clean)
    score = silhouette_score(univ_clean, kmeans.labels_)
    k = k
    Sil_coff = score
    silhouette_coefficients.append([k, Sil_coff])

silhouette_coefficients

sorted(silhouette_coefficients, reverse = True, key = lambda x: x[1])


# silhouette coefficients shows the number of clusters 'k = 2' as the best value

# Building KMeans clustering
bestmodel = KMeans(n_clusters = 2)
result = bestmodel.fit(univ_clean)

# ## Save the KMeans Clustering Model
# import pickle
pickle.dump(result, open('Clust_Univ.pkl', 'wb'))

import os
os.getcwd()

# Cluster labels
bestmodel.labels_

mb = pd.Series(bestmodel.labels_) 

# Concate the Results with data
df_clust = pd.concat([mb, df.Univ, df1], axis = 1)
df_clust = df_clust.rename(columns = {0:'cluster_id'})
df_clust.head()

# Aggregate using the mean of each cluster
cluster_agg = df_clust.iloc[:, 3:].groupby(df_clust.cluster_id).mean()
cluster_agg

# Save the Results to a CSV file
df_clust.to_csv('University.csv', encoding = 'utf-8', index = False)
 
import os
os.getcwd()



